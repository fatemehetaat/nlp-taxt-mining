{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc929b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bd857d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install nltk pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af962a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# Download NLTK models (first time only)\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e4f792",
   "metadata": {},
   "source": [
    "# word counts, sentence counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e29828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# === AI Essay File ===\n",
    "with open(r\"C:\\Users\\fatim\\OneDrive\\Bilder\\Skrivebord\\text mining\\AI_essays.txt\", encoding='utf-8') as file:\n",
    "    ai_text = file.read()\n",
    "\n",
    "# Match essay blocks\n",
    "ai_essay_matches = re.findall(r'(Essay\\d+)(.*?)((?=Essay\\d+)|$)', ai_text, re.DOTALL)\n",
    "\n",
    "print(f\"[AI] Total essays found: {len(ai_essay_matches)}\")\n",
    "\n",
    "data_ai = []\n",
    "for i, (essay_label, essay_text, _) in enumerate(ai_essay_matches, 1):\n",
    "    essay_text = essay_text.strip()\n",
    "    sentences = sent_tokenize(essay_text)\n",
    "    words = word_tokenize(essay_text)\n",
    "    clean_words = [word for word in words if word.isalpha()]\n",
    "    \n",
    "    sentence_count = len(sentences)\n",
    "    word_count_clean = len(clean_words)\n",
    "    avg_words_per_sentence = word_count_clean / sentence_count if sentence_count else 0\n",
    "    ttr = len(set([w.lower() for w in clean_words])) / len(clean_words) if clean_words else 0\n",
    "\n",
    "    data_ai.append({\n",
    "        'Essay #': i,\n",
    "        'Essay Label': essay_label,\n",
    "        'Sentence Count': sentence_count,\n",
    "        'Word Count (Clean)': word_count_clean,\n",
    "        'Avg Words/Sentence': round(avg_words_per_sentence, 2),\n",
    "        'TTR (Clean)': round(ttr, 4)\n",
    "    })\n",
    "\n",
    "df_ai = pd.DataFrame(data_ai)\n",
    "df_ai.to_csv('ai_essays_basic_stats_clean.csv', index=False)\n",
    "print(\"\\n[AI Essays]\")\n",
    "print(df_ai.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23a132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Human Essay File ===\n",
    "with open(r\"C:\\Users\\fatim\\OneDrive\\Bilder\\Skrivebord\\text mining\\human_essays.txt\", encoding='utf-8') as file:\n",
    "    human_text = file.read()\n",
    "\n",
    "# Match essay blocks\n",
    "human_essay_matches = re.findall(r'(Essay\\d+)(.*?)((?=Essay\\d+)|$)', human_text, re.DOTALL)\n",
    "\n",
    "print(f\"[Human] Total essays found: {len(human_essay_matches)}\")\n",
    "\n",
    "data_human = []\n",
    "for i, (essay_label, essay_text, _) in enumerate(human_essay_matches, 1):\n",
    "    essay_text = essay_text.strip()\n",
    "    sentences = sent_tokenize(essay_text)\n",
    "    words = word_tokenize(essay_text)\n",
    "    clean_words = [word for word in words if word.isalpha()]\n",
    "    \n",
    "    sentence_count = len(sentences)\n",
    "    word_count_clean = len(clean_words)\n",
    "    avg_words_per_sentence = word_count_clean / sentence_count if sentence_count else 0\n",
    "    ttr = len(set([w.lower() for w in clean_words])) / len(clean_words) if clean_words else 0\n",
    "\n",
    "    data_human.append({\n",
    "        'Essay #': i,\n",
    "        'Essay Label': essay_label,\n",
    "        'Sentence Count': sentence_count,\n",
    "        'Word Count (Clean)': word_count_clean,\n",
    "        'Avg Words/Sentence': round(avg_words_per_sentence, 2),\n",
    "        'TTR (Clean)': round(ttr, 4)\n",
    "    })\n",
    "\n",
    "df_human = pd.DataFrame(data_human)\n",
    "df_human.to_csv('human_essays_basic_stats_clean.csv', index=False)\n",
    "print(\"\\n[Human Essays]\")\n",
    "print(df_human.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cc06ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate means for AI\n",
    "means = df_ai[['Sentence Count', 'Word Count (Clean)', 'Avg Words/Sentence']].mean()\n",
    "stds = df_ai[['Sentence Count', 'Word Count (Clean)', 'Avg Words/Sentence']].std()\n",
    "\n",
    "print(\"Means:\\n\", means.round(2))\n",
    "print(\"Standard Deviations:\\n\", stds.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ec1b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate means for human\n",
    "means = df_human[['Sentence Count', 'Word Count (Clean)', 'Avg Words/Sentence']].mean()\n",
    "stds = df_human[['Sentence Count', 'Word Count (Clean)', 'Avg Words/Sentence']].std()\n",
    "\n",
    "print(\"Means:\\n\", means.round(2))\n",
    "print(\"Standard Deviations:\\n\", stds.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a6745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure consistent style\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Define features\n",
    "features = ['Sentence Count', 'Word Count (Clean)', 'Avg Words/Sentence']\n",
    "titles = ['Sentence Count Distribution', 'Word Count Distribution', 'Avg. Words per Sentence Distribution']\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot each feature\n",
    "for i, feature in enumerate(features):\n",
    "    sns.histplot(df_ai[feature], color='blue', label='AI Essays', kde=True, stat=\"density\", ax=axes[i], bins=15, alpha=0.5)\n",
    "    sns.histplot(df_human[feature], color='orange', label='Human Essays', kde=True, stat=\"density\", ax=axes[i], bins=15, alpha=0.5)\n",
    "    axes[i].set_title(titles[i])\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel('Density')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4130fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Define features and titles\n",
    "features = ['Sentence Count', 'Word Count (Clean)', 'Avg Words/Sentence']\n",
    "titles = [\n",
    "    'Sentence Count Distribution',\n",
    "    'Word Count Distribution',\n",
    "    'Avg. Words per Sentence Distribution'\n",
    "]\n",
    "\n",
    "# Create 3 rows and 1 column of subplots\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 15))  # Tall figure\n",
    "\n",
    "# Plot each feature in a separate row\n",
    "for i, feature in enumerate(features):\n",
    "    sns.histplot(df_ai[feature], color='blue', label='AI Essays', kde=True, stat=\"density\", ax=axes[i], bins=15, alpha=0.5)\n",
    "    sns.histplot(df_human[feature], color='orange', label='Human Essays', kde=True, stat=\"density\", ax=axes[i], bins=15, alpha=0.5)\n",
    "    axes[i].set_title(titles[i], fontsize=14)\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel('Density')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab5bc3b",
   "metadata": {},
   "source": [
    "# Lexical diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53ca08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lexicalrichness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85848b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# === Human Essay File ===\n",
    "with open(r\"C:\\Users\\fatim\\OneDrive\\Bilder\\Skrivebord\\text mining\\human_essays.txt\", encoding='utf-8') as file:\n",
    "    human_text = file.read()\n",
    "\n",
    "# Match essay blocks\n",
    "human_essay_matches = re.findall(r'(Essay\\d+)(.*?)((?=Essay\\d+)|$)', human_text, re.DOTALL)\n",
    "\n",
    "print(f\"[Human] Total essays found: {len(human_essay_matches)}\")\n",
    "\n",
    "data_human = []\n",
    "for i, (essay_label, essay_text, _) in enumerate(human_essay_matches, 1):\n",
    "    essay_text = essay_text.strip()\n",
    "    sentences = sent_tokenize(essay_text)\n",
    "    words = word_tokenize(essay_text)\n",
    "    clean_words = [word for word in words if word.isalpha()]\n",
    "    \n",
    "    sentence_count = len(sentences)\n",
    "    word_count_clean = len(clean_words)\n",
    "    avg_words_per_sentence = word_count_clean / sentence_count if sentence_count else 0\n",
    "    ttr = len(set([w.lower() for w in clean_words])) / len(clean_words) if clean_words else 0\n",
    "\n",
    "    data_human.append({\n",
    "        'Essay #': i,\n",
    "        'Essay Label': essay_label,\n",
    "        'Essay Text': essay_text,              # <- full raw text\n",
    "        'Clean_Words': clean_words,            # <- tokenized clean words\n",
    "        'Sentence Count': sentence_count,\n",
    "        'Word Count (Clean)': word_count_clean,\n",
    "        'Avg Words/Sentence': round(avg_words_per_sentence, 2),\n",
    "        'TTR (Clean)': round(ttr, 4)\n",
    "    })\n",
    "\n",
    "df_human = pd.DataFrame(data_human)\n",
    "df_human.to_csv('human_essays_basic_stats_clean.csv', index=False)\n",
    "print(\"\\n[Human Essays]\")\n",
    "print(df_human.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f31e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# === AI Essay File ===\n",
    "with open(r\"C:\\Users\\fatim\\OneDrive\\Bilder\\Skrivebord\\text mining\\AI_essays.txt\", encoding='utf-8') as file:\n",
    "    ai_text = file.read()\n",
    "\n",
    "# Match essay blocks\n",
    "ai_essay_matches = re.findall(r'(Essay\\d+)(.*?)((?=Essay\\d+)|$)', ai_text, re.DOTALL)\n",
    "\n",
    "print(f\"[AI] Total essays found: {len(ai_essay_matches)}\")\n",
    "\n",
    "data_ai = []\n",
    "for i, (essay_label, essay_text, _) in enumerate(ai_essay_matches, 1):\n",
    "    essay_text = essay_text.strip()\n",
    "    sentences = sent_tokenize(essay_text)\n",
    "    words = word_tokenize(essay_text)\n",
    "    clean_words = [word for word in words if word.isalpha()]\n",
    "    \n",
    "    sentence_count = len(sentences)\n",
    "    word_count_clean = len(clean_words)\n",
    "    avg_words_per_sentence = word_count_clean / sentence_count if sentence_count else 0\n",
    "    ttr = len(set([w.lower() for w in clean_words])) / len(clean_words) if clean_words else 0\n",
    "\n",
    "    data_ai.append({\n",
    "        'Essay #': i,\n",
    "        'Essay Label': essay_label,\n",
    "        'Essay Text': essay_text,              # <- full raw text\n",
    "        'Clean_Words': clean_words,            # <- tokenized clean words\n",
    "        'Sentence Count': sentence_count,\n",
    "        'Word Count (Clean)': word_count_clean,\n",
    "        'Avg Words/Sentence': round(avg_words_per_sentence, 2),\n",
    "        'TTR (Clean)': round(ttr, 4)\n",
    "    })\n",
    "\n",
    "df_ai = pd.DataFrame(data_ai)\n",
    "df_ai.to_csv('ai_essays_basic_stats_clean.csv', index=False)\n",
    "print(\"\\n[AI Essays]\")\n",
    "print(df_ai.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6093cadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average TTR for AI essays\n",
    "avg_ttr_ai = df_ai['TTR (Clean)'].mean()\n",
    "print(f\"Average TTR for AI essays: {avg_ttr_ai:.4f}\")\n",
    "\n",
    "# Calculate average TTR for Human essays\n",
    "avg_ttr_human = df_human['TTR (Clean)'].mean()\n",
    "print(f\"Average TTR for Human essays: {avg_ttr_human:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0b6e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.kdeplot(df_ai['TTR (Clean)'], color='green', label='AI Essays', fill=True, alpha=0.5)\n",
    "sns.kdeplot(df_human['TTR (Clean)'], color='yellow', label='Human Essays', fill=True, alpha=0.5)\n",
    "\n",
    "plt.title('TTR (Lexical Diversity) Distribution')\n",
    "plt.xlabel('TTR (Clean)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb5a6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df_ai['Word Count (Clean)'], df_ai['TTR (Clean)'], color='blue', label='AI', alpha=0.6)\n",
    "plt.scatter(df_human['Word Count (Clean)'], df_human['TTR (Clean)'], color='orange', label='Human', alpha=0.6)\n",
    "plt.xlabel('Word Count (Clean)')\n",
    "plt.ylabel('TTR (Clean)')\n",
    "plt.title('TTR vs. Word Count')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b762ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lexical-diversity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552973bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexical_diversity import lex_div as ld\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a5d8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For AI Essays\n",
    "df_ai['MTLD'] = df_ai['Clean_Words'].apply(ld.mtld)\n",
    "df_ai['MSTTR'] = df_ai['Clean_Words'].apply(ld.msttr)\n",
    "\n",
    "# For Human Essays\n",
    "df_human['MTLD'] = df_human['Clean_Words'].apply(ld.mtld)\n",
    "df_human['MSTTR'] = df_human['Clean_Words'].apply(ld.msttr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6310c3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AI Essays - Avg MTLD:\", round(df_ai['MTLD'].mean(), 2))\n",
    "print(\"AI Essays - Avg MSTTR:\", round(df_ai['MSTTR'].mean(), 2))\n",
    "\n",
    "print(\"Human Essays - Avg MTLD:\", round(df_human['MTLD'].mean(), 2))\n",
    "print(\"Human Essays - Avg MSTTR:\", round(df_human['MSTTR'].mean(), 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3d0376",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AI Columns:\", df_ai.columns.duplicated().any())\n",
    "print(\"Human Columns:\", df_human.columns.duplicated().any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bcf3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ai = df_ai.loc[:, ~df_ai.columns.duplicated()]\n",
    "df_human = df_human.loc[:, ~df_human.columns.duplicated()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4267f2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ai['Type'] = 'AI'\n",
    "df_human['Type'] = 'Human'\n",
    "combined_df = pd.concat([df_ai, df_human], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b50681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Combine into one DataFrame for plotting\n",
    "df_ai['Type'] = 'AI'\n",
    "df_human['Type'] = 'Human'\n",
    "combined_df = pd.concat([df_ai, df_human], ignore_index=True)\n",
    "\n",
    "# Plot MTLD\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(data=combined_df, x='Type', y='MTLD', palette='Set2')\n",
    "plt.title('MTLD Comparison Between AI and Human Essays')\n",
    "plt.ylabel('MTLD (Lexical Diversity)')\n",
    "plt.xlabel('')\n",
    "plt.show()\n",
    "\n",
    "# Plot MSTTR\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(data=combined_df, x='Type', y='MSTTR', palette='Set2')\n",
    "plt.title('MSTTR Comparison Between AI and Human Essays')\n",
    "plt.ylabel('MSTTR (Lexical Diversity)')\n",
    "plt.xlabel('')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56d96db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set plot style\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Define figure\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 10), sharex=False)\n",
    "\n",
    "# === MTLD Plot ===\n",
    "sns.histplot(data=combined_df, x='MTLD', hue='Type', kde=True, bins=20,\n",
    "             palette='Set2', ax=axes[0], stat=\"density\", alpha=0.6)\n",
    "axes[0].set_title('MTLD Distribution: AI vs Human Essays')\n",
    "axes[0].set_xlabel('MTLD Score')\n",
    "axes[0].set_ylabel('Density')\n",
    "\n",
    "# === MSTTR Plot ===\n",
    "sns.histplot(data=combined_df, x='MSTTR', hue='Type', kde=True, bins=20,\n",
    "             palette='Set1', ax=axes[1], stat=\"density\", alpha=0.6)\n",
    "axes[1].set_title('MSTTR Distribution: AI vs Human Essays')\n",
    "axes[1].set_xlabel('MSTTR Score')\n",
    "axes[1].set_ylabel('Density')\n",
    "\n",
    "# Layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590709fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(data=combined_df, x='MTLD', y='Type', inner=None, palette='Set2', linewidth=1)\n",
    "sns.stripplot(data=combined_df, x='MTLD', y='Type', color='black', size=4, jitter=0.25, alpha=0.6)\n",
    "plt.title('MTLD RainCloud-style Distribution')\n",
    "plt.xlabel('MTLD Score')\n",
    "plt.ylabel('')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00f7a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign topics based on index: every 10 essays per topic\n",
    "topic_labels = [f\"Topic {i+1}\" for i in range(8) for _ in range(10)]\n",
    "\n",
    "# Ensure both dataframes have correct length\n",
    "df_ai['Topic'] = topic_labels\n",
    "df_human['Topic'] = topic_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18715c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "combined_df = pd.concat([df_ai, df_human], ignore_index=True)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.boxplot(data=combined_df, x='Topic', y='MTLD', hue='Type', palette='Set2')\n",
    "plt.title('MTLD by Topic and Essay Type')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('MTLD')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939ec286",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "sns.boxplot(data=combined_df, x='Topic', y='MSTTR', hue='Type', palette='Set3')\n",
    "plt.title('MSTTR by Topic and Essay Type')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('MSTTR')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d7da32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute averages per Topic and Type\n",
    "topic_avgs = combined_df.groupby(['Topic', 'Type'])[['MTLD', 'MSTTR']].mean().round(2).reset_index()\n",
    "\n",
    "print(\"Average MTLD and MSTTR per Topic and Essay Type:\")\n",
    "print(topic_avgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d66fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall averages\n",
    "overall_avgs = combined_df.groupby('Type')[['MTLD', 'MSTTR']].mean().round(2).reset_index()\n",
    "\n",
    "print(\"\\nOverall Averages for AI and Human Essays:\")\n",
    "print(overall_avgs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827c4695",
   "metadata": {},
   "source": [
    "# syntactic complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4076c967",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43984604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc155b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def syntactic_complexity(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "\n",
    "    if len(sentences) == 0:\n",
    "        return {\n",
    "            'Mean Sentence Length': 0,\n",
    "            'Mean Clauses per Sentence': 0,\n",
    "            'Complex T-unit Ratio': 0\n",
    "        }\n",
    "\n",
    "    total_tokens = sum(len([t for t in sent if not t.is_punct]) for sent in sentences)\n",
    "    mean_sentence_length = total_tokens / len(sentences)\n",
    "\n",
    "    clauses = [token for token in doc if token.dep_ in ('ccomp', 'xcomp', 'advcl', 'relcl', 'acl')]\n",
    "    clauses_per_sentence = len(clauses) / len(sentences)\n",
    "\n",
    "    t_units = [sent for sent in sentences if any(tok.dep_ == 'ROOT' for tok in sent)]\n",
    "    complex_t_units = [t for t in t_units if any(tok.dep_ in ('advcl', 'ccomp', 'xcomp', 'relcl') for tok in t)]\n",
    "    complex_t_unit_ratio = len(complex_t_units) / len(t_units) if t_units else 0\n",
    "\n",
    "    return {\n",
    "        'Mean Sentence Length': round(mean_sentence_length, 2),\n",
    "        'Mean Clauses per Sentence': round(clauses_per_sentence, 2),\n",
    "        'Complex T-unit Ratio': round(complex_t_unit_ratio, 2)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6322897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For AI essays\n",
    "syntactic_ai = df_ai['Essay Text'].apply(syntactic_complexity)\n",
    "df_ai_complexity = pd.DataFrame(list(syntactic_ai))\n",
    "df_ai = pd.concat([df_ai, df_ai_complexity], axis=1)\n",
    "\n",
    "# For Human essays\n",
    "syntactic_human = df_human['Essay Text'].apply(syntactic_complexity)\n",
    "df_human_complexity = pd.DataFrame(list(syntactic_human))\n",
    "df_human = pd.concat([df_human, df_human_complexity], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f379b1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AI Essays - Syntactic Complexity (mean):\")\n",
    "print(df_ai[['Mean Sentence Length', 'Mean Clauses per Sentence', 'Complex T-unit Ratio']].mean().round(2))\n",
    "\n",
    "print(\"\\nHuman Essays - Syntactic Complexity (mean):\")\n",
    "print(df_human[['Mean Sentence Length', 'Mean Clauses per Sentence', 'Complex T-unit Ratio']].mean().round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bc7069",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ai['Type'] = 'AI'\n",
    "df_human['Type'] = 'Human'\n",
    "combined_df = pd.concat([df_ai, df_human], ignore_index=True)\n",
    "\n",
    "features = ['Mean Sentence Length', 'Mean Clauses per Sentence', 'Complex T-unit Ratio']\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "for i, feature in enumerate(features, 1):\n",
    "    plt.subplot(3, 1, i)\n",
    "    sns.boxplot(data=combined_df, x='Type', y=feature, palette='Set2')\n",
    "    plt.title(f'{feature} by Essay Type')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794108e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede12c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df_ai and df_human contain these columns\n",
    "df_ai['Type'] = 'AI'\n",
    "df_human['Type'] = 'Human'\n",
    "\n",
    "# Combine into one DataFrame\n",
    "combined = pd.concat([df_ai, df_human], ignore_index=True)\n",
    "\n",
    "# Define syntactic features\n",
    "features = ['Mean Sentence Length', 'Mean Clauses per Sentence', 'Complex T-unit Ratio']\n",
    "colors = ['grey', 'skyblue', 'yellow']  \n",
    "\n",
    "# Set up the plot grid\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    sns.violinplot(data=combined, x='Type', y=feature, palette=[colors[i]]*2,\n",
    "                   inner=None, ax=axes[i])\n",
    "    sns.boxplot(data=combined, x='Type', y=feature, width=0.1,\n",
    "                boxprops=dict(alpha=0.6), ax=axes[i], showfliers=False)\n",
    "    sns.stripplot(data=combined, x='Type', y=feature, color='black', size=3,\n",
    "                  jitter=0.2, alpha=0.3, ax=axes[i])\n",
    "\n",
    "    axes[i].set_title(f'{feature}', fontsize=13)\n",
    "    axes[i].grid(True)\n",
    "\n",
    "plt.suptitle('Syntactic Complexity Comparison: AI vs Human Essays', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d60e555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure 'Type' is assigned\n",
    "df_ai['Type'] = 'AI'\n",
    "df_human['Type'] = 'Human'\n",
    "\n",
    "# Combine data\n",
    "combined_df = pd.concat([df_ai, df_human], ignore_index=True)\n",
    "\n",
    "# Melt for FacetGrid\n",
    "melted_df = pd.melt(combined_df,\n",
    "                    id_vars='Type',\n",
    "                    value_vars=['Mean Sentence Length', 'Mean Clauses per Sentence', 'Complex T-unit Ratio'],\n",
    "                    var_name='Syntactic Feature',\n",
    "                    value_name='Value')\n",
    "\n",
    "# Custom colors\n",
    "palette = {'AI': '#4c72b0', 'Human': '#dd8452'}\n",
    "\n",
    "# Create vertically stacked histograms\n",
    "g = sns.FacetGrid(melted_df, row='Syntactic Feature', hue='Type',\n",
    "                  height=4, aspect=2, palette=palette, sharex=False, sharey=False)\n",
    "\n",
    "g.map(sns.histplot,\n",
    "      'Value',\n",
    "      kde=True,\n",
    "      bins=30,\n",
    "      stat='density',\n",
    "      common_norm=False,\n",
    "      alpha=0.6,\n",
    "      element='step')\n",
    "\n",
    "# Add legend and titles\n",
    "g.add_legend(title='Essay Type')\n",
    "g.set_titles('{row_name}')\n",
    "g.set_axis_labels('Value', 'Density')\n",
    "g.fig.subplots_adjust(top=0.9)\n",
    "g.fig.suptitle('Syntactic Complexity Feature Distributions by Essay Type', fontsize=16)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de876a82",
   "metadata": {},
   "source": [
    "# Discourse markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e471c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# --- 1. Define full DM list (flattened) ---\n",
    "dm_categories = {\n",
    "    \"Addition\": [\"and\", \"also\", \"plus\", \"furthermore\", \"moreover\", \"in addition\", \"additionally\", \"not only\", \"besides\"],\n",
    "    \"Contrast\": [\"but\", \"yet\", \"however\", \"on the other hand\", \"conversely\", \"nevertheless\", \"although\", \"even though\", \"while\", \"whereas\", \"nonetheless\"],\n",
    "    \"Cause and Effect\": [\"so\", \"because\", \"since\", \"therefore\", \"consequently\", \"as a result\", \"hence\", \"thus\", \"due to\", \"owing to\"],\n",
    "    \"Emphasis\": [\"indeed\", \"in fact\", \"especially\", \"particularly\", \"clearly\", \"obviously\", \"undoubtedly\", \"importantly\"],\n",
    "    \"Clarification\": [\"i mean\", \"in other words\", \"that is\", \"namely\", \"to put it another way\", \"to clarify\"],\n",
    "    \"Illustration\": [\"for example\", \"for instance\", \"such as\", \"like\", \"namely\", \"specifically\"],\n",
    "    \"Sequence\": [\"first\", \"next\", \"then\", \"lastly\", \"finally\", \"after that\", \"subsequently\", \"meanwhile\", \"simultaneously\", \"thereafter\"],\n",
    "    \"Conclusion\": [\"in conclusion\", \"to sum up\", \"in summary\", \"all in all\", \"overall\", \"to conclude\", \"in short\"],\n",
    "    \"Opinion\": [\"in my opinion\", \"i believe\", \"i think\", \"personally\", \"it seems to me\", \"from my perspective\", \"as far as i’m concerned\"],\n",
    "    \"Time\": [\"now\", \"today\", \"yesterday\", \"last year\", \"last summer\", \"recently\", \"currently\", \"previously\", \"soon\", \"later\", \"until\", \"whenever\", \"years ago\", \"as long as\", \"by the time\"],\n",
    "    \"Condition\": [\"if\", \"unless\", \"provided that\", \"in case\", \"as long as\", \"supposing\", \"on condition that\"],\n",
    "    \"Agreement\": [\"of course\", \"definitely\", \"surely\", \"indeed\", \"naturally\", \"by all means\"],\n",
    "    \"Disagreement\": [\"however\", \"on the contrary\", \"in contrast\", \"nonetheless\"],\n",
    "    \"Limitation\": [\"but\", \"yet\", \"although\", \"even though\", \"however\"],\n",
    "    \"Possibility\": [\"maybe\", \"perhaps\", \"possibly\", \"potentially\", \"could\", \"might\", \"can\", \"may\", \"would\"]\n",
    "}\n",
    "\n",
    "# Flatten to a unique list (preserve multi-word DMs)\n",
    "all_dms = sorted(set(phrase.lower() for phrases in dm_categories.values() for phrase in phrases))\n",
    "\n",
    "# --- 2. Helper function to count DMs in a list of texts ---\n",
    "def count_discourse_markers(texts):\n",
    "    marker_counts = Counter()\n",
    "    \n",
    "    for text in texts:\n",
    "        text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Generate n-grams (1 to 4) for multi-word markers\n",
    "        for n in range(1, 5):\n",
    "            for gram in ngrams(tokens, n):\n",
    "                gram_text = \" \".join(gram)\n",
    "                if gram_text in all_dms:\n",
    "                    marker_counts[gram_text] += 1\n",
    "                    \n",
    "    return marker_counts\n",
    "\n",
    "# --- 3. Run on your DataFrames ---\n",
    "# Make sure 'Essay Text' or equivalent column name exists\n",
    "texts_ai = df_ai['Essay Text'].dropna().tolist()\n",
    "texts_human = df_human['Essay Text'].dropna().tolist()\n",
    "\n",
    "ai_dm_counts = count_discourse_markers(texts_ai)\n",
    "human_dm_counts = count_discourse_markers(texts_human)\n",
    "\n",
    "# --- 4. Combine and show as list ---\n",
    "total_dm = pd.DataFrame({\n",
    "    'Discourse Marker': sorted(set(ai_dm_counts.keys()).union(human_dm_counts.keys())),\n",
    "    'AI Count': [ai_dm_counts.get(dm, 0) for dm in sorted(set(ai_dm_counts.keys()).union(human_dm_counts.keys()))],\n",
    "    'Human Count': [human_dm_counts.get(dm, 0) for dm in sorted(set(ai_dm_counts.keys()).union(human_dm_counts.keys()))]\n",
    "})\n",
    "\n",
    "# Sort by total usage\n",
    "total_dm['Total'] = total_dm['AI Count'] + total_dm['Human Count']\n",
    "total_dm = total_dm.sort_values(by='Total', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display top 20 (or export to CSV)\n",
    "print(total_dm.head(20))\n",
    "# total_dm.to_csv(\"discourse_marker_frequencies.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5346c688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_dm_phrases(texts, dm_list):\n",
    "    counts = Counter()\n",
    "    for text in texts:\n",
    "        text_lower = text.lower()\n",
    "        for dm in dm_list:\n",
    "            dm_lower = dm.lower()\n",
    "            # Match whole phrases with boundaries (so \"like\" doesn't match \"likely\")\n",
    "            pattern = r'\\b' + re.escape(dm_lower) + r'\\b'\n",
    "            matches = re.findall(pattern, text_lower)\n",
    "            counts[dm_lower] += len(matches)\n",
    "    return counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dac530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All discourse markers (flattened)\n",
    "all_dms = [dm for sublist in dm_categories.values() for dm in sublist]\n",
    "\n",
    "# Ensure you're passing full essay text list (not tokenized words)\n",
    "ai_texts = df_ai['Essay Text'].tolist()\n",
    "human_texts = df_human['Essay Text'].tolist()\n",
    "\n",
    "# Count discourse markers\n",
    "ai_dm_counts = count_dm_phrases(ai_texts, all_dms)\n",
    "human_dm_counts = count_dm_phrases(human_texts, all_dms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eb5ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_usage = []\n",
    "for dm in sorted(set(dm.lower() for dm in all_dms)):\n",
    "    ai_count = ai_dm_counts.get(dm, 0)\n",
    "    human_count = human_dm_counts.get(dm, 0)\n",
    "    total = ai_count + human_count\n",
    "    if total > 0:  # Only show used DMs\n",
    "        dm_usage.append({'Discourse Marker': dm, 'AI Count': ai_count, 'Human Count': human_count, 'Total': total})\n",
    "\n",
    "dm_df = pd.DataFrame(dm_usage).sort_values(by='Total', ascending=False).reset_index(drop=True)\n",
    "print(dm_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e82b24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Group marker counts by category ---\n",
    "category_totals = []\n",
    "\n",
    "for category, markers in dm_categories.items():\n",
    "    ai_sum = sum(ai_dm_counts.get(marker.lower(), 0) for marker in markers)\n",
    "    human_sum = sum(human_dm_counts.get(marker.lower(), 0) for marker in markers)\n",
    "    total = ai_sum + human_sum\n",
    "    category_totals.append({\n",
    "        'Category': category,\n",
    "        'AI Count': ai_sum,\n",
    "        'Human Count': human_sum,\n",
    "        'Total': total\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "category_df = pd.DataFrame(category_totals)\n",
    "category_df = category_df.sort_values(by='Total', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Show or export\n",
    "print(category_df)\n",
    "# category_df.to_csv('dm_category_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9813dc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'Category': ['Addition', 'Contrast', 'Limitation', 'Possibility', 'Cause and Effect',\n",
    "                 'Sequence', 'Illustration', 'Time', 'Condition', 'Opinion',\n",
    "                 'Emphasis', 'Disagreement', 'Clarification', 'Agreement', 'Conclusion'],\n",
    "    'AI Count': [884, 253, 231, 190, 113, 100, 91, 80, 57, 32, 28, 9, 4, 5, 1],\n",
    "    'Human Count': [905, 221, 218, 205, 82, 94, 101, 75, 54, 58, 17, 0, 3, 2, 1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Plot settings\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.set(style=\"whitegrid\")\n",
    "bar_plot = sns.barplot(\n",
    "    x='Category', \n",
    "    y='value', \n",
    "    hue='variable', \n",
    "    data=pd.melt(df, ['Category']),\n",
    "    palette='Set2'\n",
    ")\n",
    "\n",
    "# Styling\n",
    "plt.title('Discourse Marker Category Usage: AI vs Human', fontsize=16, weight='bold')\n",
    "plt.xlabel('Discourse Marker Category', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Source')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea5fb98",
   "metadata": {},
   "source": [
    "# readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c073291",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textstat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9d7c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "\n",
    "def calculate_readability_scores(df):\n",
    "    flesch = []\n",
    "    fk_grade = []\n",
    "    fog = []\n",
    "    smog = []\n",
    "    ari = []\n",
    "\n",
    "    for text in df['Essay Text']:\n",
    "        flesch.append(textstat.flesch_reading_ease(text))\n",
    "        fk_grade.append(textstat.flesch_kincaid_grade(text))\n",
    "        fog.append(textstat.gunning_fog(text))\n",
    "        smog.append(textstat.smog_index(text))\n",
    "        ari.append(textstat.automated_readability_index(text))\n",
    "    \n",
    "    df['Flesch Reading Ease'] = flesch\n",
    "    df['Flesch-Kincaid Grade'] = fk_grade\n",
    "    df['Gunning Fog'] = fog\n",
    "    df['SMOG'] = smog\n",
    "    df['ARI'] = ari\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1e6916",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ai = calculate_readability_scores(df_ai)\n",
    "df_human = calculate_readability_scores(df_human)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d8fc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Add labels\n",
    "df_ai['Type'] = 'AI'\n",
    "df_human['Type'] = 'Human'\n",
    "combined_df = pd.concat([df_ai, df_human], ignore_index=True)\n",
    "\n",
    "# Choose readability metrics to visualize\n",
    "readability_metrics = ['Flesch Reading Ease', 'Flesch-Kincaid Grade', 'Gunning Fog']\n",
    "\n",
    "# Plot each metric vertically\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(10, 12))\n",
    "\n",
    "for i, metric in enumerate(readability_metrics):\n",
    "    sns.boxplot(x='Type', y=metric, data=combined_df, ax=axes[i], palette='pastel')\n",
    "    axes[i].set_title(f'{metric} Comparison')\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel(metric)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96101c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Readability Scores:\\n\")\n",
    "\n",
    "for metric in readability_metrics:\n",
    "    ai_avg = df_ai[metric].mean()\n",
    "    human_avg = df_human[metric].mean()\n",
    "    print(f\"{metric}: AI = {ai_avg:.2f}, Human = {human_avg:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
